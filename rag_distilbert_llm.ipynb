{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOOSBqipEYROoTtSGdIaE7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MPRaghava/SmartStoreNET/blob/master/rag_distilbert_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the dependecies"
      ],
      "metadata": {
        "id": "T3MnS8puRCyG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jdzRnfkA9E_B",
        "outputId": "ac56408b-53ed-4b35-b370-1fec5df8412c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.15)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.142)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2, faiss-cpu\n",
            "Successfully installed PyPDF2-3.0.1 faiss-cpu-1.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain transformers faiss-cpu PyPDF2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKmBxtKhJF1N",
        "outputId": "8f3f4f5e-e5ed-43f0-b368-8c1f0e5db282"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Loading and Text extraction.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jUown2IoApjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "  reader = PdfReader(pdf_path)\n",
        "  text = \"\"\n",
        "  for page in reader.pages:\n",
        "    text += page.extract_text()\n",
        "  return text\n",
        "\n",
        "\n",
        "pdf_text = extract_text_from_pdf(\"/Active Retrieval Augmented Generation.pdf\")"
      ],
      "metadata": {
        "id": "x_mjAe0E9Y8Z"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdf_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9x9SgkIfAaNY",
        "outputId": "4c6327ed-90db-4225-c341-e418695be978"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Active Retrieval Augmented Generation\n",
            "Zhengbao Jiang1∗Frank F. Xu1∗Luyu Gao1∗Zhiqing Sun1∗Qian Liu2\n",
            "Jane Dwivedi-Yu3Yiming Yang1Jamie Callan1Graham Neubig1\n",
            "1Language Technologies Institute, Carnegie Mellon University\n",
            "2Sea AI Lab3FAIR, Meta\n",
            "{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu\n",
            "Abstract\n",
            "Despite the remarkable ability of large lan-\n",
            "guage models (LMs) to comprehend and gen-\n",
            "erate language, they have a tendency to hal-\n",
            "lucinate and create factually inaccurate out-\n",
            "put. Augmenting LMs by retrieving informa-\n",
            "tion from external knowledge resources is one\n",
            "promising solution. Most existing retrieval aug-\n",
            "mented LMs employ a retrieve-and-generate\n",
            "setup that only retrieves information once based\n",
            "on the input. This is limiting, however, in\n",
            "more general scenarios involving generation\n",
            "of long texts, where continually gathering in-\n",
            "formation throughout generation is essential. In\n",
            "this work, we provide a generalized view of ac-\n",
            "tive retrieval augmented generation , methods\n",
            "that actively decide when and what to retrieve\n",
            "across the course of the generation. We propose\n",
            "Forward- Looking Active REtrieval augmented\n",
            "generation ( FLARE ), a generic method which\n",
            "iteratively uses a prediction of the upcoming\n",
            "sentence to anticipate future content, which is\n",
            "then utilized as a query to retrieve relevant doc-\n",
            "uments to regenerate the sentence if it contains\n",
            "low-confidence tokens. We test FLARE along\n",
            "with baselines comprehensively over 4 long-\n",
            "form knowledge-intensive generation tasks/-\n",
            "datasets. FLARE achieves superior or compet-\n",
            "itive performance on all tasks, demonstrating\n",
            "the effectiveness of our method.1\n",
            "1 Introduction\n",
            "Generative language models (LMs) (Brown et al.,\n",
            "2020; Ouyang et al., 2022; OpenAI, 2023; Chowd-\n",
            "hery et al., 2022; Zhang et al., 2022; Touvron et al.,\n",
            "2023; Zhao et al., 2023) have become a founda-\n",
            "tional component in natural language processing\n",
            "(NLP) systems with their remarkable abilities. Al-\n",
            "though LMs have memorized some world knowl-\n",
            "edge during training (Petroni et al., 2019; Roberts\n",
            "et al., 2020; Jiang et al., 2020), they still tend to\n",
            "∗Lead contributors.\n",
            "1Code and datasets are available at https://github.com/\n",
            "jzbjyb/FLARE .hallucinate and create imaginary content (Maynez\n",
            "et al., 2020; Zhou et al., 2021). Augmenting LMs\n",
            "with retrieval components that look up relevant in-\n",
            "formation from external knowledge resources is a\n",
            "promising direction to address hallucination (Khan-\n",
            "delwal et al., 2020; Izacard et al., 2022).\n",
            "Retrieval augmented LMs commonly use a\n",
            "retrieve-and-generate setup where they retrieve doc-\n",
            "uments based on the user’s input, and then generate\n",
            "a complete answer conditioning on the retrieved\n",
            "documents (Chen et al., 2017; Guu et al., 2020;\n",
            "Lewis et al., 2020; Izacard and Grave, 2021; Sachan\n",
            "et al., 2021; Lee et al., 2021; Jiang et al., 2022;\n",
            "Izacard et al., 2022; Nakano et al., 2021; Qian\n",
            "et al., 2023; Lazaridou et al., 2022; Shi et al., 2023).\n",
            "These single-time retrieval augmented LMs outper-\n",
            "form purely parametric LMs, particularly for short-\n",
            "form knowledge-intensive generation tasks such\n",
            "as factoid question answering (QA) (Kwiatkowski\n",
            "et al., 2019; Joshi et al., 2017), where the informa-\n",
            "tion needs are clear in the user’s input, and it is\n",
            "sufficient to retrieve relevant knowledge once solely\n",
            "based on the input .\n",
            "Increasingly powerful large LMs have also\n",
            "demonstrated abilities in more complex tasks that\n",
            "involve generating long-form output, such as long-\n",
            "form QA (Fan et al., 2019; Stelmakh et al., 2022),\n",
            "open-domain summarization (Cohen et al., 2021;\n",
            "Hayashi et al., 2021; Giorgi et al., 2022), and\n",
            "(chain-of-thought; CoT) reasoning (Wei et al.,\n",
            "2022; Ho et al., 2020; Geva et al., 2021; Hendrycks\n",
            "et al., 2020). In contrast to short-form generation,\n",
            "long-form generation presents complex informa-\n",
            "tion needs that are not always evident from the in-\n",
            "put alone . Similar to how humans gradually gather\n",
            "information as we create content such as papers,\n",
            "essays, or books, long-form generation with LMs\n",
            "would require gathering multiple pieces of knowl-\n",
            "edge throughout the generation process . For ex-\n",
            "ample, to generate a summary about a particular\n",
            "topic, the initial retrieval based on the topic namearXiv:2305.06983v2  [cs.CL]  22 Oct 2023Generate a summary about Joe Biden.Search results:   !![1]: …[2]: …\n",
            "Joe Biden (born November 20, 1942) is the 46th president of the United States.Joe Biden (born November 20, 1942) is the 46th president of the United States.He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science.Joe Biden attended the University of Pennsylvania, where he earned a law degree.RetrieverInputStep 1Search results:   !\"![1]: …[2]: …\"####\"#$#$$Step 2\n",
            "Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019.Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019.\"#%#%Step 3Search results:   !\"\"[1]: …[2]: …RetrieveddocumentsLMGeneration$%$%%\n",
            "Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\n",
            "input xand initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n",
            "italic ) and check whether it contains low-probability tokens (indicated with underline ). If so (step 2 and 3), the\n",
            "system retrieves relevant documents and regenerates the sentence.\n",
            "(e.g., Joe Biden) may not cover all aspects and de-\n",
            "tails. It is crucial to retrieve extra information as\n",
            "needed during generation, such as when generat-\n",
            "ing a certain aspect (e.g., Joe Biden’s education\n",
            "history) or a specific detail (e.g., the date of Joe\n",
            "Biden’s presidential campaign announcement).\n",
            "Several attempts have been made to retrieve mul-\n",
            "tiple times throughout generation. These attempts\n",
            "include methods that passively use the past context\n",
            "to retrieve additional information at a fixed interval\n",
            "(Khandelwal et al., 2020; Borgeaud et al., 2022;\n",
            "Ram et al., 2023; Trivedi et al., 2022) which might\n",
            "not accurately reflect what LMs intend to gener-\n",
            "ate in the future or retrieve at inappropriate points.\n",
            "Some works in multihop QA decompose the full\n",
            "question into sub-questions, each of which is used\n",
            "to retrieve extra information (Press et al., 2022; Yao\n",
            "et al., 2022; Khot et al., 2022; Khattab et al., 2022).\n",
            "We ask the following question: can we create a\n",
            "simple and generic retrieval augmented LM that ac-\n",
            "tively decides when and what to retrieve throughout\n",
            "the generation process, and are applicable to a va-\n",
            "riety of long-form generation tasks? We provide a\n",
            "generalized view of active retrieval augmented gen-\n",
            "eration. Our hypothesis regarding when to retrieve\n",
            "is that LMs should retrieve information only whenthey lack the required knowledge to avoid unneces-\n",
            "sary or inappropriate retrieval that occurs in passive\n",
            "retrieval augmented LMs (Khandelwal et al., 2020;\n",
            "Borgeaud et al., 2022; Ram et al., 2023; Trivedi\n",
            "et al., 2022). Given the observation that large LMs\n",
            "tend to be well-calibrated and low probability/con-\n",
            "fidence often indicates a lack of knowledge (Ka-\n",
            "davath et al., 2022), we adopt an active retrieval\n",
            "strategy that only retrieves when LMs generate low-\n",
            "probability tokens. When deciding what to retrieve ,\n",
            "it is important to consider what LMs intend to gen-\n",
            "erate in the future, as the goal of active retrieval is to\n",
            "benefit future generations. Therefore, we propose\n",
            "anticipating the future by generating a temporary\n",
            "next sentence, using it as a query to retrieve rel-\n",
            "evant documents, and then regenerating the next\n",
            "sentence conditioning on the retrieved documents.\n",
            "Combining the two aspects, we propose Forward-\n",
            "Looking Active REtrieval augmented generation\n",
            "(FLARE ), as illustrated in Figure 1. FLARE iter-\n",
            "atively generates a temporary next sentence , use\n",
            "it as the query to retrieve relevant documents if it\n",
            "contains low-probability tokens and regenerate the\n",
            "next sentence until reaches the end.\n",
            "FLARE is applicable to any existing LMs at\n",
            "inference time without additional training. Con-sidering the impressive performance achieved by\n",
            "GPT-3.5 (Ouyang et al., 2022) on a variety of\n",
            "tasks, we examine the effectiveness of our meth-\n",
            "ods on text-davinci-003 . We evaluate FLARE\n",
            "on 4 diverse tasks/datasets involving generating\n",
            "long outputs, including multihop QA (2WikiMul-\n",
            "tihopQA), commonsense reasoning (StrategyQA),\n",
            "long-form QA (ASQA), and open-domain summa-\n",
            "rization (WikiAsp) (Ho et al., 2020; Geva et al.,\n",
            "2021; Stelmakh et al., 2022; Hayashi et al., 2021).\n",
            "Over all tasks, FLARE achieves superior or com-\n",
            "petitive performance compared to single-time and\n",
            "multi-time retrieval baselines, demonstrating the\n",
            "effectiveness and generalizability of our method.\n",
            "2 Retrieval Augmented Generation\n",
            "We formally define single-time retrieval augmented\n",
            "generation and propose the framework of active\n",
            "retrieval augmented generation.\n",
            "2.1 Notations and Definitions\n",
            "Given a user input xand a document corpus D=\n",
            "{di}|D|\n",
            "i=1(such as all Wikipedia articles), the goal of\n",
            "retrieval augmented LMs is to generate the answer\n",
            "y= [s1,s2, ...,sm] = [w1, w2, ..., w n]containing\n",
            "msentences or ntokens leveraging information\n",
            "retrieved from the corpus.\n",
            "In retrieval augmented LM, the LM typically\n",
            "pairs with a retriever that can retrieve a list of\n",
            "documents Dq=ret(q)for a query q; the LM\n",
            "conditions on both the user input xand retrieved\n",
            "documents Dqto generate the answer. Since we\n",
            "focus on examining various methods of determin-\n",
            "ing when and what to retrieve, we follow exist-\n",
            "ing methods (Ram et al., 2023; Trivedi et al.,\n",
            "2022) to prepend the retrieved documents before\n",
            "the user input to aid future generation for both\n",
            "baselines and our method for fair comparisons:\n",
            "y=LM([Dq,x]), where [·,·]is concatenation fol-\n",
            "lowing the specified order.\n",
            "2.2 Single-time Retrieval Augmented\n",
            "Generation\n",
            "The most common choice is to directly use the user\n",
            "input as the query for retrieval and generate the\n",
            "complete answer at once y=LM([Dx,x]).\n",
            "2.3 Active Retrieval Augmented Generation\n",
            "To aid long-form generation with retrieval, we pro-\n",
            "pose active retrieval augmented generation. It is a\n",
            "generic framework that actively decides when and\n",
            "what to retrieve through the generation process,resulting in the interleaving of retrieval and genera-\n",
            "tion. Formally, at step t(t≥1), the retrieval query\n",
            "qtis formulated based on both the user input xand\n",
            "previously generated output y<t= [y0, ...,yt−1]:\n",
            "qt=qry(x,y<t),\n",
            "where qry(·)is the query formulation function. At\n",
            "the beginning ( t= 1), the previous generation is\n",
            "empty ( y<1=∅), and the user input is used as the\n",
            "initial query ( q1=x). Given retrieved documents\n",
            "Dqt, LMs continually generate the answer until the\n",
            "next retrieval is triggered or reaches the end:\n",
            "yt=LM([Dqt,x,y<t]),\n",
            "where ytrepresents the generated tokens at the cur-\n",
            "rent step t, and the input to LMs is the concatena-\n",
            "tion of the retrieved documents Dqt, the user input\n",
            "x, and the previous generation y<t. We discard\n",
            "previously retrieved documents ∪t′<tDqt′and only\n",
            "use the retrieved documents from the current step\n",
            "to condition the next generation to prevent reaching\n",
            "the input length limit of LMs.\n",
            "3 FLARE: Forward-Looking Active\n",
            "REtrieval Augmented Generation\n",
            "Our intuition is that (1) LMs should only retrieve\n",
            "information when they do not have the necessary\n",
            "knowledge to avoid unnecessary or inappropriate\n",
            "retrieval, and (2) the retrieval queries should reflect\n",
            "the intents of future generations. We propose two\n",
            "forward-looking active retrieval augmented gener-\n",
            "ation (FLARE) methods to implement the active\n",
            "retrieval augmented generation framework. The\n",
            "first method prompts the LM to generate retrieval\n",
            "queries when necessary while generating the an-\n",
            "swer using retrieval-encouraging instructions, de-\n",
            "noted as FLARE instruct . The second method directly\n",
            "uses the LM’s generation as search queries, denoted\n",
            "as FLARE direct, which iteratively generates the next\n",
            "sentence to gain insight into the future topic, and\n",
            "if uncertain tokens are present, retrieves relevant\n",
            "documents to regenerate the next sentence.\n",
            "3.1 FLARE with Retrieval Instructions\n",
            "Inspired by Toolformer (Schick et al., 2023), a\n",
            "straightforward way of expressing information\n",
            "needs for retrieval is to generate “[Search(query)]”\n",
            "when additional information is needed (Schick\n",
            "et al., 2023), e.g., “The colors on the flag of\n",
            "Ghana have the following meanings. Red is for\n",
            "[Search(Ghana flag red meaning)] the blood of mar-\n",
            "tyrs, ...” When working with GPT-3.5 models thatSearch results:   !![1]: …[2]: …Joe Biden attended\n",
            "Search results:   !\"![1]: …[2]: …Search results:   !\"\"[1]: …[2]: …[Search(Joe Biden University)][Search(Joe Biden degree)]the University of Pennsylvania, where he earneda law degree.Generate a summary about Joe Biden.Input$&$&#%$&%%%GenerationRetriever$%$%%Figure 2: An illustration of forward-looking active re-\n",
            "trieval augmented generation with retrieval instructions\n",
            "(FLARE instruct ). It iteratively generates search queries\n",
            "(shown in gray italic ) to retrieve relevant information to\n",
            "aid future generations.\n",
            "offer only API access, we elicit such behavior by\n",
            "few-shot prompting (Brown et al., 2020).\n",
            "Specifically, for a downstream task, we place\n",
            "the search-related instruction and exemplars at the\n",
            "beginning as skill 1, followed by the instruction and\n",
            "exemplars of the downstream task as skill 2. Given\n",
            "a test case, we ask LMs to combine skills 1 and 2 to\n",
            "generate search queries while performing the task.\n",
            "The structure of the prompt is shown in Prompt 3.1,\n",
            "and full details can be found in Prompt D.3.\n",
            "Prompt 3.1: retrieval instructions\n",
            "Skill 1. An instruction to guide LMs to generate search\n",
            "queries.\n",
            "Several search-related exemplars.\n",
            "Skill 2. An instruction to guide LMs to perform a\n",
            "specific downstream task (e.g., multihop QA).\n",
            "Several task-related exemplars.\n",
            "An instruction to guide LMs to combine skills 1\n",
            "and 2 for the test case.\n",
            "The input of the test case.\n",
            "As shown in Figure 2, when the LM generates\n",
            "“[Search(query)]” (shown in gray italic ), we stop\n",
            "the generation and use the query terms to retrieve\n",
            "relevant documents, which are prepended before\n",
            "the user input to aid future generation until the\n",
            "next search query is generated or reaches the end.\n",
            "Additional implementation details are included in\n",
            "Appendix A.\n",
            "3.2 Direct FLARE\n",
            "Since we cannot fine-tune black-box LMs, we\n",
            "found queries generated by FLARE instruct throughretrieval instructions might not be reliable. There-\n",
            "fore, we propose a more direct way of forward-\n",
            "looking active retrieval that uses the next sentence\n",
            "to decide when and what to retrieve.\n",
            "3.2.1 Confidence-based Active Retrieval\n",
            "As shown in Figure 1, at step t, we first generate a\n",
            "temporary next sentence ˆst=LM([x,y<t])with-\n",
            "out conditioning on retrieved documents. Then we\n",
            "decide whether to trigger retrieval and formulate\n",
            "queries based on ˆst. If the LM is confident about ˆst,\n",
            "we accept it without retrieving additional informa-\n",
            "tion; if not, we use ˆstto formulate search queries\n",
            "qtto retrieve relevant documents, and then regen-\n",
            "erate the next sentence st. The reason we utilize\n",
            "sentences as the basis of our iteration is due to their\n",
            "significance as semantic units that are neither too\n",
            "short nor too lengthy like phrases and paragraphs.\n",
            "However, our approach can also utilize phrases or\n",
            "paragraphs as the basis.\n",
            "Since LMs tend to be well-calibrated that low\n",
            "probability/confidence often indicates a lack of\n",
            "knowledge (Jiang et al., 2021; Kadavath et al.,\n",
            "2022; Varshney et al., 2022), we actively trigger\n",
            "retrieval if any token of ˆsthas a probability lower\n",
            "than a threshold θ∈[0,1].θ= 0means retrieval\n",
            "is never triggered, while θ= 1 triggers retrieval\n",
            "every sentence.\n",
            "yt=(\n",
            "ˆst if all tokens of ˆsthave probs ≥θ\n",
            "st=LM([Dqt,x,y<t]) otherwise\n",
            "where the query qtis formulated based on ˆst.\n",
            "3.2.2 Confidence-based Query Formulation\n",
            "One way to perform retrieval is to directly use the\n",
            "next sentence ˆstas the query qt. This shares a sim-\n",
            "ilar spirit with methods that use generated hypo-\n",
            "thetical titles or paragraphs from LMs as retrieval\n",
            "queries or evidences (Gao et al., 2022; Sun et al.,\n",
            "2022; Yu et al., 2022; Mao et al., 2021). We gen-\n",
            "eralize such techniques to long-form generation\n",
            "where active information access is essential.\n",
            "We found retrieving with the next sentence\n",
            "achieves significantly better results than with the\n",
            "previous context, as shown later in subsection 6.2.\n",
            "However, it has a risk of perpetuating errors con-\n",
            "tained in it. For example, if the LM produces the\n",
            "sentence “Joe Biden attended the University of\n",
            "Pennsylvania” instead of the correct fact that he\n",
            "attended the University of Delaware, using this er-\n",
            "roneous sentence as a query might retrieve mislead-Joe Biden attended the University of Pennsylvania, where he earned a law degree.Ask a question to which the answer is “the University of Pennsylvania”Ask a question to which the answer is “a law degree”What university did Joe Biden attend?What degree did Joe Biden earn?implicit query by maskingexplicit query by question generationJoe Biden attended  , where he earned  .LM such as ChatGPTFigure 3: Implicit and explicit query formulation. To-\n",
            "kens with low probabilities are marked with underlines .\n",
            "ing information. We propose two simple methods\n",
            "to overcome this issue as illustrated in Figure 3.\n",
            "Masked sentences as implicit queries. The first\n",
            "method masks out low-confidence tokens in ˆstwith\n",
            "probabilities below a threshold β∈[0,1], where a\n",
            "higher βresults in more aggressive masking. This\n",
            "removes potential distractions from the sentence to\n",
            "improve retrieval accuracy.\n",
            "Generated questions as explicit queries. An-\n",
            "other method is to generate explicit questions that\n",
            "target the low-confident span in ˆst. For example, if\n",
            "the LM is uncertain about “the University of Penn-\n",
            "sylvania”, a question like “Which university did\n",
            "Joe Biden attend?” can help retrieve relevant in-\n",
            "formation. Self-ask (Press et al., 2022) achieved\n",
            "this by manually inserting follow-up questions\n",
            "into downstream task exemplars as shown later\n",
            "in Prompt D.2, which requires task-specific annota-\n",
            "tion efforts. Instead, we developed a universal ap-\n",
            "proach that generates questions for low-confidence\n",
            "spans without additional annotation. Specifically,\n",
            "We first extract all spans from ˆstwith probabilities\n",
            "below β. For each extracted span z, we prompt\n",
            "gpt-3.5-turbo to generate a question qt,zthat\n",
            "can be answered with the span:\n",
            "Prompt 3.2: zero-shot question generation\n",
            "User input x.\n",
            "Generated output so far y≤t.\n",
            "Given the above passage, ask a question to which\n",
            "the answer is the term/entity/phrase “ z”.\n",
            "We retrieve using each generated question and\n",
            "interleave the returned documents into a single\n",
            "ranking list to aid future generations. In summary,queries qtare formulated based on ˆstas follows:\n",
            "qt=(\n",
            "∅ if all tokens of ˆsthave probs ≥θ\n",
            "mask(ˆst)or qgen (ˆst) otherwise\n",
            "3.3 Implementation Details\n",
            "Base LM We validate our method on one of the\n",
            "most advanced GPT-3.5 LMs text-davinci-003\n",
            "by iteratively querying their API.2\n",
            "Document corpus and retrievers. Since we fo-\n",
            "cus on the integration of retrieval and generation,\n",
            "we use off-the-shelf retrievers that take queries\n",
            "as inputs and return a list of relevant documents.\n",
            "For datasets that mainly rely on knowledge from\n",
            "Wikipedia, we use the Wikipedia dump from\n",
            "Karpukhin et al. (2020) and employ BM25 (Robert-\n",
            "son and Zaragoza, 2009) as the retriever. For\n",
            "datasets that rely on knowledge from the open web,\n",
            "we use the Bing search engine as our retriever.3\n",
            "Retrieved document formatting. Multiple re-\n",
            "trieved documents are linearized according to their\n",
            "ranking and then added to the beginning of the user\n",
            "input using Prompt D.1.\n",
            "Other implementation details such as sentence to-\n",
            "kenization and efficiency are included Appendix A.\n",
            "4 Multi-time Retrieval Baselines\n",
            "Existing passive multi-time retrieval augmented\n",
            "LMs can also be formulated using our framework\n",
            "(subsection 2.3). In this section, we formally in-\n",
            "troduce three baseline categories based on when\n",
            "and what to retrieve. These baselines are not exact\n",
            "reproductions of the corresponding paper because\n",
            "many design choices differ which makes direct\n",
            "comparisons impossible. We implemented them\n",
            "using the same settings, with the only variation\n",
            "being when and what to retrieve.\n",
            "Previous-window approaches trigger retrieval\n",
            "every ltokens, where lrepresents the window size.\n",
            "Generated tokens from the previous window are\n",
            "used as the query:\n",
            "qt=yt−1(t≥2),\n",
            "yt= [w(t−1)l+1, ..., w tl].\n",
            "Some existing methods in this category are RETRO\n",
            "(Borgeaud et al., 2022), IC-RALM (Ram et al.,\n",
            "2https://api.openai.com/v1/completions April 23.\n",
            "3https://www.microsoft.com/en-us/bing/apis/\n",
            "bing-web-search-api2023), which retrieve every few tokens, and KNN-\n",
            "LM (Khandelwal et al., 2020), which retrieves ev-\n",
            "ery token.4We follow Ram et al. (2023) to use a\n",
            "window size of l= 16 .\n",
            "Previous-sentence approaches trigger retrieval\n",
            "every sentence and use the previous sentence as the\n",
            "query, and IRCoT (Trivedi et al., 2022) belongs to\n",
            "this category:\n",
            "qt=yt−1(t≥2),\n",
            "yt=st.\n",
            "Question decomposition approaches manually\n",
            "annotated task-specific exemplars to guide LMs\n",
            "to generate decomposed sub-questions while pro-\n",
            "ducing outputs. For example, self-ask (Press et al.,\n",
            "2022), a method in this category, manually inserts\n",
            "sub-questions in exemplars using Prompt D.2. For\n",
            "the test case, retrieval is triggered dynamically\n",
            "whenever the model generates a sub-question.\n",
            "The aforementioned approaches can retrieve ad-\n",
            "ditional information while generating. However,\n",
            "they have notable drawbacks: (1) Using previously\n",
            "generated tokens as queries might not reflect what\n",
            "LMs intend to generate in the future. (2) Retriev-\n",
            "ing information at a fixed interval can be inefficient\n",
            "because it might occur at inappropriate points. (3)\n",
            "Question decomposition approaches require task-\n",
            "specific prompt engineering, which restricts their\n",
            "generalizability in new tasks.\n",
            "5 Experimental Setup\n",
            "We evaluate the effectiveness of FLARE on 4 di-\n",
            "verse knowledge-intensive tasks using few-shot in-\n",
            "context learning (Radford et al., 2019; Brown et al.,\n",
            "2020; Liu et al., 2023). We follow previous works\n",
            "(Trivedi et al., 2022) to sub-sample at most 500\n",
            "examples from each dataset due to the cost of run-\n",
            "ning experiments. Datasets, metrics, and settings\n",
            "are summarized in Table 7 of Appendix B. The\n",
            "hyperparameters of FLARE are selected based on\n",
            "the development set and listed in Table 9. FLARE\n",
            "refers to FLARE direct if not specifically stated.\n",
            "Multihop QA The goal of multihop QA is to\n",
            "answer complex questions through information re-\n",
            "trieval and reasoning. We use 2WikiMultihopQA\n",
            "(Ho et al., 2020) which contains 2-hop complex\n",
            "4Since KNN-LM uses the contextualized representation\n",
            "corresponding to the current decoding position to retrieve rel-\n",
            "evant information which encodes all previous tokens. Strictly\n",
            "speaking, qtshould be y<t.questions sourced from Wikipedia articles that re-\n",
            "quire composition, comparison, or inference, e.g.,\n",
            "“Why did the founder of Versus die?” We follow\n",
            "Wang et al. (2022) to generate both the chain-of-\n",
            "thought and the final answer. Experimental setting\n",
            "details are included in Appendix B.\n",
            "We use regular expressions to extract the final\n",
            "answer from the output and compare it with the ref-\n",
            "erence answer using exact match (EM), and token-\n",
            "level F 1, precision, and recall.\n",
            "Commonsense reasoning Commonsense reason-\n",
            "ing requires world and commonsense knowledge\n",
            "to generate answers. We use StrategyQA (Geva\n",
            "et al., 2021) which is a collection of crowdsourced\n",
            "yes/no questions, e.g., “Would a pear sink in wa-\n",
            "ter?” We follow Wei et al. (2022) to generate both\n",
            "the chain-of-thought and the final yes/no answer.\n",
            "Details are included in Appendix B.\n",
            "We extract the final answer and match it against\n",
            "the gold answer using exact match.\n",
            "Long-form QA Long-form QA aims to generate\n",
            "comprehensive answers to questions seeking com-\n",
            "plex information (Fan et al., 2019; Stelmakh et al.,\n",
            "2022). We use ASQA (Stelmakh et al., 2022) as our\n",
            "testbed where inputs are ambiguous questions with\n",
            "multiple interpretations, and outputs should cover\n",
            "all of them. For example, “Where do the Philadel-\n",
            "phia Eagles play their home games?” could be\n",
            "asking about the city, sports complex, or stadium.\n",
            "We found in many cases it is challenging even for\n",
            "humans to identify which aspect of the question\n",
            "is ambiguous. Therefore, we created another set-\n",
            "ting (ASQA-hint) where we provide a brief hint\n",
            "to guide LMs to stay on track when generating an-\n",
            "swers. The hint for the above case is “This question\n",
            "is ambiguous in terms of which specific location or\n",
            "venue is being referred to.” Experimental setting\n",
            "details are included in Appendix B.\n",
            "We use metrics from Stelmakh et al. (2022), in-\n",
            "cluding EM, RoBERTa-based QA score (Disambig-\n",
            "F1), ROUGE (Lin, 2004), and an overall score com-\n",
            "bining Disambig-F 1and ROUGE (DR).\n",
            "Open-domain summarization The goal of open-\n",
            "domain summarization is to generate a comprehen-\n",
            "sive summary about a topic by gathering informa-\n",
            "tion from open web (Giorgi et al., 2022). We use\n",
            "WikiAsp (Hayashi et al., 2021) which aims to gen-\n",
            "erate aspect-based summaries about entities from\n",
            "20 domains in Wikipedia, e.g., “Generate a sum-\n",
            "mary about Echo School (Oregon) including the0.020.040.060.080.0\n",
            "2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAspNo ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for\n",
            "each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.\n",
            "following aspects: academics, history.” Experimen-\n",
            "tal setting details are included in Appendix B.\n",
            "Metrics include ROUGE, named entity-based F 1,\n",
            "and UniEval (Zhong et al., 2022) which measures\n",
            "factual consistency.\n",
            "6 Experimental Results\n",
            "We first report overall results across 4 tasks/datasets\n",
            "and compare the performance of FLARE with all\n",
            "the baselines introduced in section 4. We then\n",
            "run ablation experiments to study the efficacy of\n",
            "various design choices of our method.\n",
            "6.1 Comparison with Baselines\n",
            "Overall results. The overall performance of\n",
            "FLARE and baseline across all tasks/datasets are\n",
            "reported in Figure 4. FLARE outperforms all base-\n",
            "line on all tasks/datasets, indicating that FLARE\n",
            "is a generic method that can effectively retrieve\n",
            "additional information throughout the generation.\n",
            "Among various tasks, multihop QA shows the\n",
            "most significant improvement. This is largely due\n",
            "to the task’s clear definition and specific objective\n",
            "of producing the final answer through a 2-hop rea-\n",
            "soning process, which makes it easier for LMs to\n",
            "generate on-topic output. In contrast, ASQA and\n",
            "WikiAsp are more open-ended, which increases the\n",
            "difficulty of both generation and evaluation. The\n",
            "improvement on ASQA-hint is larger than that of\n",
            "ASQA because identifying ambiguous aspects is\n",
            "challenging even for humans in many cases, and\n",
            "providing a generic hint helps LMs to stay on topic.\n",
            "Thorough comparisons with baselines. The per-\n",
            "formance of all baselines on 2WikiMultihopQA\n",
            "are reported in Table 1. FLARE outperforms all\n",
            "baselines by a large margin, which confirms that\n",
            "forward-looking active retrieval is highly effective.\n",
            "Most multi-time retrieval augmented approaches\n",
            "outperform single-time retrieval but with differentMethods EM F 1 Prec. Rec.\n",
            "No retrieval 28.2 36.8 36.5 38.6\n",
            "Single-time retrieval 39.4 48.8 48.6 51.5\n",
            "Multi-time retrieval\n",
            "Previous-window 43.2 52.3 51.7 54.5\n",
            "Previous-sentence 39.0 49.2 48.9 51.8\n",
            "Question decomposition 47.8 56.4 56.1 58.6\n",
            "FLARE instruct (ours) 42.4 49.8 49.1 52.5\n",
            "FLARE direct(ours) 51.0 59.7 59.1 62.6\n",
            "Table 1: FLARE and baselines on 2WikiMultihopQA.\n",
            "Previous-window (Borgeaud et al., 2022; Ram et al.,\n",
            "2023), previous-sentence (Trivedi et al., 2022), and ques-\n",
            "tion decomposition (Press et al., 2022; Yao et al., 2022)\n",
            "methods are reimplemented for fair comparisons.\n",
            "margins. The improvement of retrieving using the\n",
            "previous sentence is relatively small which we hy-\n",
            "pothesize is mainly because the previous sentence\n",
            "often describes entities or relations different from\n",
            "those in the next sentence in 2WikiMultihopQA.\n",
            "While the previous-window approach might use\n",
            "the first half of a sentence to retrieve information\n",
            "potentially helpful for generating the second half.\n",
            "Among all baselines, the question decomposition\n",
            "approach (Press et al., 2022) achieves the best per-\n",
            "formance. which is not surprising since the in-\n",
            "context exemplars manually annotated with decom-\n",
            "posed sub-questions (Prompt D.2) guide LMs to\n",
            "generate sub-questions that align with the topic/in-\n",
            "tent of future generations. FLARE outperforms\n",
            "this baseline, indicating that manual exemplar an-\n",
            "notation is not necessary for effective future-aware\n",
            "retrieval. The gap between FLARE instruct and ques-\n",
            "tion decomposition is large, indicating that teaching\n",
            "LMs to generate search queries using task-generic\n",
            "retrieval instructions and exemplars is challenging.\n",
            "We report all metrics for the other datasets in\n",
            "Table 2. FLARE outperforms baselines with re-\n",
            "spect to all metrics. Retrieval using the previ-Datasets StrategyQA ASQA ASQA-hint WikiAsp\n",
            "Metrics EM EM D-F 1R-L DR EM D-F 1R-L DR UniEval E-F 1R-L\n",
            "No retrieval 72.9 33.8 24.2 33.3 28.4 40.1 32.5 36.4 34.4 47.1 14.1 26.4\n",
            "Single-time retrieval 68.6 40.0 27.1 34.0 30.4 43.2 34.8 37.4 36.0 52.4 17.4 26.9\n",
            "Multi-time retrieval\n",
            "Previous-window 71.2 39.9 27.0 34.3 30.4 43.7 35.7 37.5 36.6 51.8 18.1 27.3\n",
            "Previous-sentence 71.0 39.9 27.9 34.3 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2\n",
            "FLARE (ours) 77.3 41.3 28.2 34.3 31.1 46.2 36.7 37.7 37.2 53.4 18.9 27.6\n",
            "Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F 1is\n",
            "Disambig-F 1, R-L is ROUGE-L, and E-F 1is named entity-based F 1.\n",
            "2WikiMultihopQA ASQA-hint\n",
            "EM F 1Prec. Rec. EM D-F 1R-L DR\n",
            "Previous 39.0 49.2 48.9 51.8 42.5 34.1 36.9 35.5\n",
            "Next 48.8 57.6 57.1 60.5 45.9 35.7 37.5 36.6\n",
            "Table 3: A head-to-head comparison between using the\n",
            "previous sentence and the next sentence for retrieval.\n",
            "#Tokens EM F 1 Prec. Rec.\n",
            "16 43.2 52.3 51.7 54.5\n",
            "32 43.6 52.4 52.0 55.0\n",
            "48 40.0 49.3 49.0 52.0\n",
            "All 39.0 48.5 48.2 51.1\n",
            "Table 4: Previous-window approaches using different\n",
            "numbers of tokens as queries.\n",
            "ous window underperforms single-time retrieval\n",
            "on ASQA, which we hypothesize is because the\n",
            "previous window does not accurately reflect future\n",
            "intent. Since we focus on evaluating factuality, met-\n",
            "rics with an emphasis on factual content (such as\n",
            "EM, Disambig-F 1, UniEval) are more reliable than\n",
            "metrics computed over all tokens (ROUGE-L).\n",
            "6.2 Ablation Study\n",
            "Importance of forward-looking retrieval. We\n",
            "first validate that forward-looking retrieval is more\n",
            "effective than past-context-based retrieval. We run\n",
            "ablation experiments on 2WikiMultihopQA and\n",
            "ASQA-hint comparing retrieval using the previ-\n",
            "ous versus the next sentence. Specifically, both\n",
            "methods retrieve every sentence and directly use\n",
            "the complete previous/next sentence as queries. As\n",
            "shown in Table 3, using the next sentence to retrieve\n",
            "is clearly better than using the previous sentence,\n",
            "confirming our hypothesis.\n",
            "We also run previous-window approaches using\n",
            "different numbers of past tokens as queries. As\n",
            "shown in Table 4, using too many tokens ( >32) in\n",
            "%steps/sentences with retrieval0.020.040.060.080.0\n",
            "0.0 25.0 50.0 75.0 100.02WikiMultihopQA StrategyQAFigure 5: Performance (EM) of FLARE with respect\n",
            "to the percentage of steps/sentences with retrieval on\n",
            "2WikiMultihopQA and StrategyQA.\n",
            "the past hurts the performance, further confirming\n",
            "our hypothesis that previous context might not be\n",
            "relevant to intent of future generations.\n",
            "Importance of active retrieval. Next, we inves-\n",
            "tigate how active retrieval threshold θaffects per-\n",
            "formance. To alter our method from not retrieving\n",
            "to retrieving every sentence, we adjust the confi-\n",
            "dence threshold θthat determines when to trigger\n",
            "retrieval from 0 to 1. We then calculate the pro-\n",
            "portion of steps/sentences where retrieval is acti-\n",
            "vated, and present the performance based on it. As\n",
            "shown in Figure 5, on 2WikiMultihopQA, the per-\n",
            "formance plateaus when the retrieval percentage\n",
            "exceeds 60%, indicating that retrieval when LMs\n",
            "are confident is not necessary. On StrategyQA, the\n",
            "performance drops when the retrieval percentage\n",
            "exceeds 50%, indicating that unnecessary retrieval\n",
            "can introduce noise and impede the original gen-\n",
            "eration process. We found triggering retrieval for\n",
            "40%-80% of sentences usually leads to a good per-\n",
            "formance across tasks/datasets.\n",
            "Effectiveness of different query formulation\n",
            "methods We study implicit query formation by\n",
            "masking and explicit query formulation through\n",
            "question generation. In Table 5, we compare the\n",
            "performance of FLARE with different maskingβ EM F 1 Prec. Rec.\n",
            "0.0 0.488 0.576 0.571 0.605\n",
            "0.2 0.498 0.588 0.582 0.616\n",
            "0.4 0.510 0.597 0.591 0.627\n",
            "0.6 0.506 0.593 0.586 0.622\n",
            "Table 5: Performance of FLARE with respect to the\n",
            "masking threshold βon 2WikiMultihopQA.\n",
            "ASQA-hint WikiAsp\n",
            "EM D-F 1R-L DR UniEval E-F 1R-L\n",
            "Implicit 45.7 36.9 37.7 37.3 53.4 18.8 27.7\n",
            "Explicit 46.2 36.7 37.7 37.2 53.4 18.9 27.6\n",
            "Table 6: A comparison between implicit and explicit\n",
            "query formulation methods in FLARE.\n",
            "thresholds β. Retrieving directly with the complete\n",
            "sentence ( β= 0) is worse than masking tokens\n",
            "with low probabilities, confirming our hypothesis\n",
            "that low-confidence erroneous tokens can distract\n",
            "retrievers. We compare implicit and explicit query\n",
            "formulation methods in Table 6. Performances of\n",
            "both methods are similar, indicating that both meth-\n",
            "ods can effectively reflect information needs.\n",
            "7 Related Work\n",
            "We refer to subsection 2.2 and section 4 for ex-\n",
            "tensively discussion on single-time and multi-time\n",
            "retrieval augmented LMs, which is the most rele-\n",
            "vant area to this paper.\n",
            "Iterative and adaptive retrieval Iterative re-\n",
            "trieval and refinement has been studied in both\n",
            "text and code generation tasks (Peng et al., 2023;\n",
            "Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n",
            "et al., 2023). FLARE differs from these methods in\n",
            "the granularity of generation and retrieval strategies.\n",
            "Adaptive retrieval has been studied in single-time\n",
            "retrieval scenarios based on either question pop-\n",
            "ularity or generation probabilities (Mallen et al.,\n",
            "2022; Li et al., 2023), while we focus on long-form\n",
            "generation requiring active information access.\n",
            "Browser-enhanced LMs WebGPT (Nakano\n",
            "et al., 2021) and WebCPM (Qin et al., 2023) train\n",
            "LMs to interact with browser to enhance factuality\n",
            "using reinforcement learning or supervised train-\n",
            "ing where multiple queries can be triggered before\n",
            "generation. FLARE is built on text-based retrievers\n",
            "but can be combined with a browser to potentially\n",
            "improve retrieval quality.8 Conclusion\n",
            "To aid long-form generation with retrieval aug-\n",
            "mentation, we propose an active retrieval aug-\n",
            "mented generation framework that decides when\n",
            "and what to retrieve during generation. We imple-\n",
            "ment this framework with forward-looking active\n",
            "retrieval that iteratively uses the upcoming sentence\n",
            "to retrieve relevant information if it contains low-\n",
            "confidence tokens and regenerates the next sen-\n",
            "tence. Experimental results on 4 tasks/datasets\n",
            "demonstrate the effectiveness of our methods. Fu-\n",
            "ture directions include better strategies for active\n",
            "retrieval and developing efficient LM architectures\n",
            "for active information integration.\n",
            "9 Limitations\n",
            "We also conduct experiments on Wizard of\n",
            "Wikipedia (Dinan et al., 2019) and ELI5 (Fan et al.,\n",
            "2019), and found that FLARE did not provide sig-\n",
            "nificant gains. Wizard of Wikipedia is a knowledge-\n",
            "intensive dialogue generation dataset where the out-\n",
            "put is relatively short ( ∼20 tokens on average) so\n",
            "retrieving multiple disparate pieces of information\n",
            "might not be necessary. ELI5 (Fan et al., 2019)\n",
            "is a long-form QA dataset requiring in-depth an-\n",
            "swers to open-ended questions. Due to issues men-\n",
            "tioned in Krishna et al. (2021) such as difficulties\n",
            "of grounding generation in retrieval and evalua-\n",
            "tion, both single-time retrieval and FLARE did not\n",
            "provide significant gains over not using retrieval.\n",
            "From an engineering perspective, interleaving gen-\n",
            "eration and retrieval with a naive implementation\n",
            "increases both overheads and the cost of generation.\n",
            "LMs need to be activated multiple times (once for\n",
            "each retrieval) and a caching-free implementation\n",
            "also requires recomputing the previous activation\n",
            "each time after retrieval. This issue can be poten-\n",
            "tially alleviated with special architectural designs\n",
            "that encode the retrieved documents Dqtand the\n",
            "input/generation ( x/y<t) independently.\n",
            "Acknowledgements\n",
            "This work was supported in part by a grant from\n",
            "the Singapore Defence Science and Technology\n",
            "Agency and the IBM PhD Fellowship. We thank\n",
            "Chunting Zhou, Amanda Bertsch, Uri Alon, Hi-\n",
            "roaki Hayashi, Harsh Trivedi, Patrick Lewis, Timo\n",
            "Schick, Kaixin Ma, Shuyan Zhou, and Songwei Ge\n",
            "for their insightful discussions and help with the\n",
            "experiments.References\n",
            "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\n",
            "Trevor Cai, Eliza Rutherford, Katie Millican, George\n",
            "van den Driessche, Jean-Baptiste Lespiau, Bogdan\n",
            "Damoc, Aidan Clark, Diego de Las Casas, Aurelia\n",
            "Guy, Jacob Menick, Roman Ring, Tom Hennigan,\n",
            "Saffron Huang, Loren Maggiore, Chris Jones, Albin\n",
            "Cassirer, Andy Brock, Michela Paganini, Geoffrey\n",
            "Irving, Oriol Vinyals, Simon Osindero, Karen Si-\n",
            "monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n",
            "2022. Improving language models by retrieving from\n",
            "trillions of tokens. In International Conference on\n",
            "Machine Learning, ICML 2022, 17-23 July 2022, Bal-\n",
            "timore, Maryland, USA , volume 162 of Proceedings\n",
            "of Machine Learning Research , pages 2206–2240.\n",
            "PMLR.\n",
            "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\n",
            "Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
            "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
            "Askell, Sandhini Agarwal, Ariel Herbert-V oss,\n",
            "Gretchen Krueger, Tom Henighan, Rewon Child,\n",
            "Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\n",
            "Clemens Winter, Christopher Hesse, Mark Chen, Eric\n",
            "Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\n",
            "Jack Clark, Christopher Berner, Sam McCandlish,\n",
            "Alec Radford, Ilya Sutskever, and Dario Amodei.\n",
            "2020. Language models are few-shot learners. In Ad-\n",
            "vances in Neural Information Processing Systems 33:\n",
            "Annual Conference on Neural Information Process-\n",
            "ing Systems 2020, NeurIPS 2020, December 6-12,\n",
            "2020, virtual .\n",
            "Danqi Chen, Adam Fisch, Jason Weston, and Antoine\n",
            "Bordes. 2017. Reading wikipedia to answer open-\n",
            "domain questions. In Proceedings of the 55th Annual\n",
            "Meeting of the Association for Computational Lin-\n",
            "guistics, ACL 2017, Vancouver, Canada, July 30 -\n",
            "August 4, Volume 1: Long Papers , pages 1870–1879.\n",
            "Association for Computational Linguistics.\n",
            "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\n",
            "Maarten Bosma, Gaurav Mishra, Adam Roberts,\n",
            "Paul Barham, Hyung Won Chung, Charles Sutton,\n",
            "Sebastian Gehrmann, Parker Schuh, Kensen Shi,\n",
            "Sasha Tsvyashchenko, Joshua Maynez, Abhishek\n",
            "Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\n",
            "odkumar Prabhakaran, Emily Reif, Nan Du, Ben\n",
            "Hutchinson, Reiner Pope, James Bradbury, Jacob\n",
            "Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\n",
            "Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\n",
            "Sunipa Dev, Henryk Michalewski, Xavier Garcia,\n",
            "Vedant Misra, Kevin Robinson, Liam Fedus, Denny\n",
            "Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\n",
            "Barret Zoph, Alexander Spiridonov, Ryan Sepassi,\n",
            "David Dohan, Shivani Agrawal, Mark Omernick, An-\n",
            "drew M. Dai, Thanumalayan Sankaranarayana Pil-\n",
            "lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\n",
            "Rewon Child, Oleksandr Polozov, Katherine Lee,\n",
            "Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\n",
            "Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\n",
            "Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\n",
            "and Noah Fiedel. 2022. Palm: Scaling language mod-\n",
            "eling with pathways. CoRR , abs/2204.02311.Nachshon Cohen, Oren Kalinsky, Yftah Ziser, and\n",
            "Alessandro Moschitti. 2021. Wikisum: Coherent\n",
            "summarization dataset for efficient human-evaluation.\n",
            "InProceedings of the 59th Annual Meeting of the As-\n",
            "sociation for Computational Linguistics and the 11th\n",
            "International Joint Conference on Natural Language\n",
            "Processing, ACL/IJCNLP 2021, (Volume 2: Short\n",
            "Papers), Virtual Event, August 1-6, 2021 , pages 212–\n",
            "219. Association for Computational Linguistics.\n",
            "Emily Dinan, Stephen Roller, Kurt Shuster, Angela\n",
            "Fan, Michael Auli, and Jason Weston. 2019. Wizard\n",
            "of wikipedia: Knowledge-powered conversational\n",
            "agents. In 7th International Conference on Learning\n",
            "Representations, ICLR 2019, New Orleans, LA, USA,\n",
            "May 6-9, 2019 . OpenReview.net.\n",
            "Angela Fan, Yacine Jernite, Ethan Perez, David Grang-\n",
            "ier, Jason Weston, and Michael Auli. 2019. ELI5:\n",
            "long form question answering. In Proceedings of\n",
            "the 57th Conference of the Association for Compu-\n",
            "tational Linguistics, ACL 2019, Florence, Italy, July\n",
            "28- August 2, 2019, Volume 1: Long Papers , pages\n",
            "3558–3567. Association for Computational Linguis-\n",
            "tics.\n",
            "Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n",
            "2022. Precise zero-shot dense retrieval without rele-\n",
            "vance labels. CoRR , abs/2212.10496.\n",
            "Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\n",
            "Dan Roth, and Jonathan Berant. 2021. Did aristotle\n",
            "use a laptop? a question answering benchmark with\n",
            "implicit reasoning strategies. Transactions of the\n",
            "Association for Computational Linguistics , 9:346–\n",
            "361.\n",
            "John M. Giorgi, Luca Soldaini, Bo Wang, Gary D.\n",
            "Bader, Kyle Lo, Lucy Lu Wang, and Arman Co-\n",
            "han. 2022. Exploring the challenges of open\n",
            "domain multi-document summarization. CoRR ,\n",
            "abs/2212.10526.\n",
            "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\n",
            "pat, and Ming-Wei Chang. 2020. REALM: retrieval-\n",
            "augmented language model pre-training. CoRR ,\n",
            "abs/2002.08909.\n",
            "Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris\n",
            "Ackerson, Raj Neervannan, and Graham Neubig.\n",
            "2021. Wikiasp: A dataset for multi-domain aspect-\n",
            "based summarization. Trans. Assoc. Comput. Lin-\n",
            "guistics , 9:211–225.\n",
            "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\n",
            "Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\n",
            "2020. Measuring massive multitask language under-\n",
            "standing. CoRR , abs/2009.03300.\n",
            "Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\n",
            "and Akiko Aizawa. 2020. Constructing A multi-hop\n",
            "QA dataset for comprehensive evaluation of reason-\n",
            "ing steps. In Proceedings of the 28th International\n",
            "Conference on Computational Linguistics, COLING\n",
            "2020, Barcelona, Spain (Online), December 8-13,2020 , pages 6609–6625. International Committee on\n",
            "Computational Linguistics.\n",
            "Gautier Izacard and Edouard Grave. 2021. Leveraging\n",
            "passage retrieval with generative models for open do-\n",
            "main question answering. In Proceedings of the 16th\n",
            "Conference of the European Chapter of the Associ-\n",
            "ation for Computational Linguistics: Main Volume,\n",
            "EACL 2021, Online, April 19 - 23, 2021 , pages 874–\n",
            "880. Association for Computational Linguistics.\n",
            "Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\n",
            "Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\n",
            "Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and\n",
            "Edouard Grave. 2022. Few-shot learning with\n",
            "retrieval augmented language models. CoRR ,\n",
            "abs/2208.03299.\n",
            "Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham\n",
            "Neubig. 2021. How can we know When language\n",
            "models know? on the calibration of language mod-\n",
            "els for question answering. Trans. Assoc. Comput.\n",
            "Linguistics , 9:962–977.\n",
            "Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,\n",
            "Zhiruo Wang, Jamie Callan, and Graham Neubig.\n",
            "2022. Retrieval as attention: End-to-end learning\n",
            "of retrieval and reading within a single transformer.\n",
            "CoRR , abs/2212.02027.\n",
            "Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\n",
            "Neubig. 2020. How can we know what language\n",
            "models know. Trans. Assoc. Comput. Linguistics ,\n",
            "8:423–438.\n",
            "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\n",
            "Zettlemoyer. 2017. Triviaqa: A large scale distantly\n",
            "supervised challenge dataset for reading comprehen-\n",
            "sion. In Proceedings of the 55th Annual Meeting of\n",
            "the Association for Computational Linguistics, ACL\n",
            "2017, Vancouver, Canada, July 30 - August 4, Volume\n",
            "1: Long Papers , pages 1601–1611. Association for\n",
            "Computational Linguistics.\n",
            "Saurav Kadavath, Tom Conerly, Amanda Askell, Tom\n",
            "Henighan, Dawn Drain, Ethan Perez, Nicholas\n",
            "Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\n",
            "Tran-Johnson, Scott Johnston, Sheer El Showk, Andy\n",
            "Jones, Nelson Elhage, Tristan Hume, Anna Chen,\n",
            "Yuntao Bai, Sam Bowman, Stanislav Fort, Deep\n",
            "Ganguli, Danny Hernandez, Josh Jacobson, Jack-\n",
            "son Kernion, Shauna Kravec, Liane Lovitt, Ka-\n",
            "mal Ndousse, Catherine Olsson, Sam Ringer, Dario\n",
            "Amodei, Tom Brown, Jack Clark, Nicholas Joseph,\n",
            "Ben Mann, Sam McCandlish, Chris Olah, and Jared\n",
            "Kaplan. 2022. Language models (mostly) know what\n",
            "they know. CoRR , abs/2207.05221.\n",
            "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\n",
            "S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\n",
            "and Wen-tau Yih. 2020. Dense passage retrieval for\n",
            "open-domain question answering. In Proceedings of\n",
            "the 2020 Conference on Empirical Methods in Nat-\n",
            "ural Language Processing, EMNLP 2020, Online,\n",
            "November 16-20, 2020 , pages 6769–6781. Associa-\n",
            "tion for Computational Linguistics.Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\n",
            "Zettlemoyer, and Mike Lewis. 2020. Generalization\n",
            "through memorization: Nearest neighbor language\n",
            "models. In 8th International Conference on Learning\n",
            "Representations, ICLR 2020, Addis Ababa, Ethiopia,\n",
            "April 26-30, 2020 . OpenReview.net.\n",
            "Omar Khattab, Keshav Santhanam, Xiang Lisa Li,\n",
            "David Hall, Percy Liang, Christopher Potts, and\n",
            "Matei Zaharia. 2022. Demonstrate-search-predict:\n",
            "Composing retrieval and language models for\n",
            "knowledge-intensive NLP. CoRR , abs/2212.14024.\n",
            "Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,\n",
            "Kyle Richardson, Peter Clark, and Ashish Sabharwal.\n",
            "2022. Decomposed prompting: A modular approach\n",
            "for solving complex tasks. CoRR , abs/2210.02406.\n",
            "Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\n",
            "Hurdles to progress in long-form question answering.\n",
            "InNorth American Association for Computational\n",
            "Linguistics .\n",
            "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n",
            "field, Michael Collins, Ankur P. Parikh, Chris Alberti,\n",
            "Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\n",
            "ton Lee, Kristina Toutanova, Llion Jones, Matthew\n",
            "Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\n",
            "Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\n",
            "ral questions: a benchmark for question answering\n",
            "research. Trans. Assoc. Comput. Linguistics , 7:452–\n",
            "466.\n",
            "Angeliki Lazaridou, Elena Gribovskaya, Wojciech\n",
            "Stokowiec, and Nikolai Grigorev. 2022. Internet-\n",
            "augmented language models through few-shot\n",
            "prompting for open-domain question answering.\n",
            "CoRR , abs/2203.05115.\n",
            "Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-\n",
            "jape, Christopher D. Manning, and Kyoung-Gu Woo.\n",
            "2021. You only need one model for open-domain\n",
            "question answering. CoRR , abs/2112.07381.\n",
            "Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\n",
            "tus, Fabio Petroni, Vladimir Karpukhin, Naman\n",
            "Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\n",
            "Tim Rocktäschel, Sebastian Riedel, and Douwe\n",
            "Kiela. 2020. Retrieval-augmented generation for\n",
            "knowledge-intensive NLP tasks. In Advances in Neu-\n",
            "ral Information Processing Systems 33: Annual Con-\n",
            "ference on Neural Information Processing Systems\n",
            "2020, NeurIPS 2020, December 6-12, 2020, virtual .\n",
            "Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang,\n",
            "Jian-Yun Nie, and Ji-Rong Wen. 2023. The web can\n",
            "be your oyster for improving large language models.\n",
            "CoRR , abs/2305.10998.\n",
            "Chin-Yew Lin. 2004. ROUGE: A package for auto-\n",
            "matic evaluation of summaries. In Text Summariza-\n",
            "tion Branches Out , pages 74–81, Barcelona, Spain.\n",
            "Association for Computational Linguistics.Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\n",
            "Hiroaki Hayashi, and Graham Neubig. 2023. Pre-\n",
            "train, prompt, and predict: A systematic survey of\n",
            "prompting methods in natural language processing.\n",
            "ACM Comput. Surv. , 55(9):195:1–195:35.\n",
            "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\n",
            "Hannaneh Hajishirzi, and Daniel Khashabi. 2022.\n",
            "When not to trust language models: Investigating\n",
            "effectiveness and limitations of parametric and non-\n",
            "parametric memories. CoRR , abs/2212.10511.\n",
            "Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong\n",
            "Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\n",
            "2021. Generation-augmented retrieval for open-\n",
            "domain question answering. In Proceedings of the\n",
            "59th Annual Meeting of the Association for Compu-\n",
            "tational Linguistics and the 11th International Joint\n",
            "Conference on Natural Language Processing, ACL/I-\n",
            "JCNLP 2021, (Volume 1: Long Papers), Virtual Event,\n",
            "August 1-6, 2021 , pages 4089–4100. Association for\n",
            "Computational Linguistics.\n",
            "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and\n",
            "Ryan McDonald. 2020. On faithfulness and factu-\n",
            "ality in abstractive summarization. In Proceedings\n",
            "of the 58th Annual Meeting of the Association for\n",
            "Computational Linguistics , pages 1906–1919, On-\n",
            "line. Association for Computational Linguistics.\n",
            "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\n",
            "Long Ouyang, Christina Kim, Christopher Hesse,\n",
            "Shantanu Jain, Vineet Kosaraju, William Saunders,\n",
            "Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\n",
            "Krueger, Kevin Button, Matthew Knight, Benjamin\n",
            "Chess, and John Schulman. 2021. Webgpt: Browser-\n",
            "assisted question-answering with human feedback.\n",
            "CoRR , abs/2112.09332.\n",
            "OpenAI. 2023. GPT-4 technical report. CoRR ,\n",
            "abs/2303.08774.\n",
            "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\n",
            "roll L. Wainwright, Pamela Mishkin, Chong Zhang,\n",
            "Sandhini Agarwal, Katarina Slama, Alex Ray, John\n",
            "Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\n",
            "Maddie Simens, Amanda Askell, Peter Welinder,\n",
            "Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022.\n",
            "Training language models to follow instructions with\n",
            "human feedback. CoRR , abs/2203.02155.\n",
            "Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\n",
            "Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\n",
            "Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check\n",
            "your facts and try again: Improving large language\n",
            "models with external knowledge and automated feed-\n",
            "back. CoRR , abs/2302.12813.\n",
            "Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,\n",
            "Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\n",
            "and Alexander H. Miller. 2019. Language mod-\n",
            "els as knowledge bases? In Proceedings of the\n",
            "2019 Conference on Empirical Methods in Natu-\n",
            "ral Language Processing and the 9th International\n",
            "Joint Conference on Natural Language Processing,EMNLP-IJCNLP 2019, Hong Kong, China, Novem-\n",
            "ber 3-7, 2019 , pages 2463–2473. Association for\n",
            "Computational Linguistics.\n",
            "Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\n",
            "Noah A Smith, and Mike Lewis. 2022. Measuring\n",
            "and narrowing the compositionality gap in language\n",
            "models. arXiv preprint arXiv:2210.03350 .\n",
            "Hongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu,\n",
            "Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao,\n",
            "Jian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain:\n",
            "Learning to generate factually correct articles for\n",
            "queries by grounding on large web corpus. CoRR ,\n",
            "abs/2304.04358.\n",
            "Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao\n",
            "Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,\n",
            "Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan\n",
            "Liu, Maosong Sun, and Jie Zhou. 2023. Webcpm: In-\n",
            "teractive web search for chinese long-form question\n",
            "answering. CoRR , abs/2305.06849.\n",
            "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\n",
            "Dario Amodei, and Ilya Sutskever. 2019. Language\n",
            "models are unsupervised multitask learners. OpenAI\n",
            "Blog , 1(8).\n",
            "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\n",
            "Amnon Shashua, Kevin Leyton-Brown, and Yoav\n",
            "Shoham. 2023. In-context retrieval-augmented lan-\n",
            "guage models. arXiv preprint arXiv:2302.00083 .\n",
            "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.\n",
            "How much knowledge can you pack into the param-\n",
            "eters of a language model? In Proceedings of the\n",
            "2020 Conference on Empirical Methods in Natural\n",
            "Language Processing, EMNLP 2020, Online, Novem-\n",
            "ber 16-20, 2020 , pages 5418–5426. Association for\n",
            "Computational Linguistics.\n",
            "Stephen E. Robertson and Hugo Zaragoza. 2009. The\n",
            "probabilistic relevance framework: BM25 and be-\n",
            "yond. Found. Trends Inf. Retr. , 3(4):333–389.\n",
            "Devendra Singh Sachan, Siva Reddy, William L. Hamil-\n",
            "ton, Chris Dyer, and Dani Yogatama. 2021. End-to-\n",
            "end training of multi-document reader and retriever\n",
            "for open-domain question answering. In Advances\n",
            "in Neural Information Processing Systems 34: An-\n",
            "nual Conference on Neural Information Processing\n",
            "Systems 2021, NeurIPS 2021, December 6-14, 2021,\n",
            "virtual , pages 25968–25981.\n",
            "Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\n",
            "Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\n",
            "Cancedda, and Thomas Scialom. 2023. Toolformer:\n",
            "Language models can teach themselves to use tools.\n",
            "Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\n",
            "Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and\n",
            "Wen-tau Yih. 2023. REPLUG: retrieval-augmented\n",
            "black-box language models. CoRR , abs/2301.12652.Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\n",
            "Wei Chang. 2022. ASQA: factoid questions meet\n",
            "long-form answers. In Proceedings of the 2022 Con-\n",
            "ference on Empirical Methods in Natural Language\n",
            "Processing, EMNLP 2022, Abu Dhabi, United Arab\n",
            "Emirates, December 7-11, 2022 , pages 8273–8288.\n",
            "Association for Computational Linguistics.\n",
            "Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\n",
            "Denny Zhou. 2022. Recitation-augmented language\n",
            "models. CoRR , abs/2210.01296.\n",
            "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n",
            "Martinet, Marie-Anne Lachaux, Timothée Lacroix,\n",
            "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\n",
            "Azhar, Aurélien Rodriguez, Armand Joulin, Edouard\n",
            "Grave, and Guillaume Lample. 2023. Llama: Open\n",
            "and efficient foundation language models. CoRR ,\n",
            "abs/2302.13971.\n",
            "Harsh Trivedi, Niranjan Balasubramanian, Tushar\n",
            "Khot, and Ashish Sabharwal. 2022. Interleav-\n",
            "ing retrieval with chain-of-thought reasoning for\n",
            "knowledge-intensive multi-step questions. CoRR ,\n",
            "abs/2212.10509.\n",
            "Neeraj Varshney, Man Luo, and Chitta Baral. 2022. Can\n",
            "open-domain QA reader utilize external knowledge\n",
            "efficiently like humans? CoRR , abs/2211.12707.\n",
            "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\n",
            "Le, Ed H. Chi, and Denny Zhou. 2022. Self-\n",
            "consistency improves chain of thought reasoning in\n",
            "language models. CoRR , abs/2203.11171.\n",
            "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n",
            "Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\n",
            "Chain of thought prompting elicits reasoning in large\n",
            "language models. CoRR , abs/2201.11903.\n",
            "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\n",
            "Shafran, Karthik Narasimhan, and Yuan Cao. 2022.\n",
            "React: Synergizing reasoning and acting in language\n",
            "models. CoRR , abs/2210.03629.\n",
            "Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\n",
            "Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,\n",
            "Michael Zeng, and Meng Jiang. 2022. Generate\n",
            "rather than retrieve: Large language models are\n",
            "strong context generators. CoRR , abs/2209.10063.\n",
            "Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng\n",
            "Jiang, and Ashish Sabharwal. 2023. Improving lan-\n",
            "guage models via plug-and-play retrieval feedback.\n",
            "CoRR , abs/2305.14002.\n",
            "Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie,\n",
            "Panupong Pasupat, Peter Shaw, Linlu Qiu, Sumit\n",
            "Sanghai, and Fei Sha. 2022. Generate-and-retrieve:\n",
            "Use your predictions to improve retrieval for seman-\n",
            "tic parsing. In Proceedings of the 29th International\n",
            "Conference on Computational Linguistics, COLING\n",
            "2022, Gyeongju, Republic of Korea, October 12-17,\n",
            "2022 , pages 4946–4951. International Committee on\n",
            "Computational Linguistics.Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang\n",
            "Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\n",
            "2023. Repocoder: Repository-level code completion\n",
            "through iterative retrieval and generation. CoRR ,\n",
            "abs/2303.12570.\n",
            "Susan Zhang, Stephen Roller, Naman Goyal, Mikel\n",
            "Artetxe, Moya Chen, Shuohui Chen, Christopher De-\n",
            "wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\n",
            "haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\n",
            "Simig, Punit Singh Koura, Anjali Sridhar, Tianlu\n",
            "Wang, and Luke Zettlemoyer. 2022. Opt: Open\n",
            "pre-trained transformer language models. ArXiv ,\n",
            "abs/2205.01068.\n",
            "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\n",
            "Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-\n",
            "ichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\n",
            "Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\n",
            "Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\n",
            "Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n",
            "2023. A survey of large language models. CoRR ,\n",
            "abs/2303.18223.\n",
            "Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\n",
            "Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\n",
            "Jiawei Han. 2022. Towards a unified multi-\n",
            "dimensional evaluator for text generation. In Pro-\n",
            "ceedings of the 2022 Conference on Empirical Meth-\n",
            "ods in Natural Language Processing, EMNLP 2022,\n",
            "Abu Dhabi, United Arab Emirates, December 7-11,\n",
            "2022 , pages 2023–2038. Association for Computa-\n",
            "tional Linguistics.\n",
            "Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab,\n",
            "Francisco Guzmán, Luke Zettlemoyer, and Marjan\n",
            "Ghazvininejad. 2021. Detecting hallucinated content\n",
            "in conditional neural sequence generation. In Find-\n",
            "ings of the Association for Computational Linguis-\n",
            "tics: ACL-IJCNLP 2021 , pages 1393–1404, Online.\n",
            "Association for Computational Linguistics.A FLARE Implementation Details\n",
            "FLARE instruct implementation details We\n",
            "found that LMs can effectively combine retrieval\n",
            "and downstream task-related skills and generate\n",
            "meaningful search queries while performing the\n",
            "task. However, there are two issues: (1) LMs tend\n",
            "to generate fewer search queries than necessary.\n",
            "(2) Generating excessive search queries can\n",
            "disrupt answer generation and adversely affect\n",
            "performance. We address these issues using two\n",
            "methods respectively. First, we increase the logit\n",
            "of the token “[” by 2.0 to improve the chances\n",
            "of LMs generating “[Search(query)]”. Second,\n",
            "whenever LMs generate a search query, we use it\n",
            "to retrieve relevant information, promptly remove\n",
            "it from the generation, and generate the next few\n",
            "tokens while forbidding “[” by adding a large\n",
            "negative value to the logit of “[”.\n",
            "The initial query of FLARE. FLARE starts\n",
            "with the user input xas the initial query to re-\n",
            "trieve documents to generate the first sentence\n",
            "ˆs1=LM([Dx,x])to bootstrap the iterative gener-\n",
            "ation process. For the following steps, the tempo-\n",
            "rary forward-looking sentence is generated without\n",
            "retrieved documents.\n",
            "Sentence tokenization. For each step t, we gen-\n",
            "erate 64 tokens which are longer than most sen-\n",
            "tences, and use NLTK sentence tokenizer5to ex-\n",
            "tract the first sentence and discard the rest.\n",
            "Efficiency As shown in subsection 6.2, on aver-\n",
            "age retrieval is triggered for 30%∼60% of sen-\n",
            "tences depending on downstream tasks. In compar-\n",
            "ision, KNN-LM (Khandelwal et al., 2020) retrieves\n",
            "every token, RETRO or IC-RALM (Borgeaud et al.,\n",
            "2022; Ram et al., 2023) retrievers every 4 ∼32 to-\n",
            "kens, and IRCoT (Trivedi et al., 2022) retrieves\n",
            "every sentence. Compared to single-time retrieval,\n",
            "however, interleaving retrieval and generation with\n",
            "a naive implementation indeed increases overheads,\n",
            "which we discuss in the limitation section (sec-\n",
            "tion 9).\n",
            "B Datasets and Settings\n",
            "Datasets, metrics, and experimental settings are\n",
            "summarized in Table 7.\n",
            "5https://www.nltk.org/api/nltk.tokenize.\n",
            "PunktSentenceTokenizer.htmlMultihop QA For “Why did the founder of Ver-\n",
            "sus die?”, the output we aim to generate is “The\n",
            "founder of Versus was Gianni Versace. Gianni Ver-\n",
            "sace was shot and killed on the steps of his Miami\n",
            "Beach mansion on July 15, 1997. So the answer\n",
            "is shot.” We use 8 exemplars from Trivedi et al.\n",
            "(2022) listed in Prompt D.4 for in-context learn-\n",
            "ing, BM25 as the retriever, and Wikipedia articles\n",
            "as the retrieval corpus. Similar to the observation\n",
            "in Trivedi et al. (2022), we found incorporating\n",
            "retrieval results for exemplars improves the per-\n",
            "formance, we use the input xof each exemplar to\n",
            "retrieve several documents and then add them using\n",
            "the format in Prompt D.1. We found increasing the\n",
            "number of retrieval documents often increases per-\n",
            "formance. Therefore, we use the maximum number\n",
            "of documents that can fit within the input length\n",
            "limit of text-davinci-003 , which is 2 for 2Wiki-\n",
            "MultihopQA.\n",
            "Commonsense Reasoning For “Would a pear\n",
            "sink in water?”, the output we aim to generate is\n",
            "“The density of a pear is about 0.6g/cm3, which is\n",
            "less than water. Objects less dense than water float.\n",
            "Thus, a pear would float. So the final answer is no.”\n",
            "We use 6 exemplars from Wei et al. (2022) listed in\n",
            "Prompt D.5, BM25 on the Wikipedia corpus, and 3\n",
            "retrieved documents to run experiments.\n",
            "Long-form QA For “Where do the Philadelphia\n",
            "Eagles play their home games?”, the output we\n",
            "aim to generate is “We need to consider the dif-\n",
            "ferent possible locations or venues that could be\n",
            "considered the home field of the Philadelphia Ea-\n",
            "gles. These include the city, the sports complex,\n",
            "or the stadium. Therefore, this question has 3 in-\n",
            "terpretations and the answers are: (1) The city is\n",
            "Philadelphia. (2) The sports complex is the South\n",
            "Philadelphia Sports Complex. (3) The stadium is\n",
            "the Lincoln Financial Field stadium.” For both the\n",
            "original setting (ASQA) and the setting with hints\n",
            "(ASQA-hint), we manually annotate 8 exemplars\n",
            "(Prompt D.6 and D.8), use BM25 on the Wikipedia\n",
            "corpus, and 3 retrieved documents to run experi-\n",
            "ments.\n",
            "Open-domain Summarization The original\n",
            "WikiAsp dataset is designed for multi-document\n",
            "summarization and provides a list of references to\n",
            "systems. We converted it into the open-domain\n",
            "setting by removing the associated references and\n",
            "instead gathering information from the open web.\n",
            "For “Generate a summary about Echo School (Ore-gon) including the following aspects: academics,\n",
            "history.”, the output we aim to generate is “# Aca-\n",
            "demics. In 2008, 91% of the school’s seniors re-\n",
            "ceived their high school diploma... # History. The\n",
            "class of 2008 was the 100th class in the school’s\n",
            "history.” where # is used to indicate aspects. We\n",
            "manually annotate 4 exemplars (Prompt D.10), and\n",
            "use the Bing search engine to retrieve 5 documents\n",
            "from the open web. To avoid leaking, we exclude\n",
            "several Wikipedia-related domains listed in Table 8\n",
            "from Bing’s search results.\n",
            "C Hyperparameters\n",
            "Hyperparameters of FLARE on different datasets\n",
            "are listed in Table 9.\n",
            "D Prompts and Few-shot exemplars\n",
            "The prompt used to linearize multiple documents\n",
            "is shown in Prompt D.1. The prompt used in self-\n",
            "ask (Press et al., 2022) is shown in Prompt D.2.\n",
            "Prompts and exemplars of different tasks/datasets\n",
            "are shown in Prompt D.3, D.4, D.5, D.6, D.8, and\n",
            "D.10, respectively.\n",
            "Prompt D.1: document formatting\n",
            "Search results:\n",
            "[1]Document 1\n",
            "[2]Document 2\n",
            "...\n",
            "The user input x\n",
            "Prompt D.2: multihop QA with self-ask\n",
            "Question: Who lived longer, Theodor Haecker or Harry\n",
            "Vaughan Watkins?\n",
            "Are follow up questions needed here: Yes.\n",
            "Follow up: How old was Theodor Haecker when he died?\n",
            "Intermediate answer: Theodor Haecker was 65 years old\n",
            "when he died.\n",
            "Follow up: How old was Harry Vaughan Watkins when he\n",
            "died?\n",
            "Intermediate answer: Harry Vaughan Watkins was 69 years\n",
            "old when he died.\n",
            "So the final answer is: Harry Vaughan Watkins.Settings 2WikiMultihopQA StrategyQA ASQA WikiAsp\n",
            "(Ho et al., 2020) (Geva et al., 2021) (Stelmakh et al., 2022) (Hayashi et al., 2021)\n",
            "Dataset statistics\n",
            "Task multihop QA commonsense QA long-form QA open-domain summarization\n",
            "#Examples 500 229 500 500\n",
            "Evaluation settings\n",
            "Metrics EM, F 1, Prec., Rec. EM EM, Disambig-F 1, ROUGE, DR UniEval, entity-F 1, ROUGE\n",
            "Retrieval settings\n",
            "Corpus Wikipedia Wikipedia Wikipedia open web\n",
            "Retriever BM25 BM25 BM25 Bing\n",
            "Top-k 2 3 3 5\n",
            "Prompt format\n",
            "#Exemplars 8 6 8 4\n",
            "Ret. for exemplars ✓ ✗ ✗ ✗\n",
            "Table 7: Dataset statistics and experimental settings of different tasks.\n",
            "wikipedia.org, wikiwand.com, wiki2.org, wikimedia.org\n",
            "Table 8: Wikipedia-related domains excluded from Bing’s search results.\n",
            "Dataset θ β Query formulation Combine single- & multi-time retrieval\n",
            "2WikiMultihopQA 0.8 0.4 implicit ✗\n",
            "StrategyQA 0.4 0.4 implicit ✗\n",
            "ASQA & ASQA-hint 0.8 0.4 explicit ✓\n",
            "WikiAsp 0.8 0.4 explicit ✓\n",
            "Table 9: Hyperparameters of FLARE on different datasets.Prompt D.3: retrieval instructions for 2WikiMultihopQA\n",
            "Skill 1. Use the Search API to look up relevant information by writing “[Search(term)]” where “term” is the search term you\n",
            "want to look up. For example:\n",
            "Question: But what are the risks during production of nanomaterials?\n",
            "Answer (with Search): [Search(nanomaterial production risks)] Some nanomaterials may give rise to various kinds of lung\n",
            "damage.\n",
            "Question: The colors on the flag of Ghana have the following meanings.\n",
            "Answer (with Search): Red is for [Search(Ghana flag red meaning)] the blood of martyrs, green for forests, and gold for\n",
            "mineral wealth.\n",
            "Question: Metformin is the first-line drug for what?\n",
            "Answer (with Search): [Search(Metformin first-line drug)] patients with type 2 diabetes and obesity.\n",
            "Skill 2. Answer questions by thinking step-by-step. First, write out the reasoning steps, then draw the conclu-\n",
            "sion. For example:\n",
            "Question: When did the director of film Hypocrite (Film) die?\n",
            "Answer (with step-by-step): The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So\n",
            "the answer is 19 June 2013.\n",
            "Question: Are both Kurram Garhi and Trojkrsti located in the same country?\n",
            "Answer (with step-by-step): Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of\n",
            "Republic of Macedonia. Thus, they are not in the same country. So the answer is no.\n",
            "Question: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same\n",
            "nationality?\n",
            "Answer (with step-by-step): Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by\n",
            "Karl Freund. David Dhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same\n",
            "nationality. So the answer is no.\n",
            "Question: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?\n",
            "Answer (with step-by-step): Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s\n",
            "father-in-law is Genghis Khan. So the answer is Genghis Khan.\n",
            "Question: Who was born first out of Martin Hodge and Ivania Martinich?\n",
            "Answer (with step-by-step): Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus,\n",
            "Martin Hodge was born first. So the answer is Martin Hodge.\n",
            "Question: When did the director of film Laughter In Hell die?\n",
            "Answer (with step-by-step): The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,\n",
            "1963. So the answer is August 25, 1963.\n",
            "Question: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?\n",
            "Answer (with step-by-step): The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took\n",
            "the West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova\n",
            "died on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\n",
            "Question: Who is the grandchild of Krishna Shah (Nepalese Royal)?\n",
            "Answer (with step-by-step): Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah.\n",
            "Thus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\n",
            "Now, combine the aforementioned two skills. First, write out the reasoning steps, then draw the conclusion,\n",
            "where the reasoning steps should also utilize the Search API “[Search(term)]” whenever possible.\n",
            "Question: Where did Minbyauk Thihapate’s wife die?\n",
            "Answer (with step-by-step & Search):Prompt D.4: exemplars of 2WikiMultihopQA\n",
            "Question: When did the director of film Hypocrite (Film) die?\n",
            "Answer: The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So the answer is 19\n",
            "June 2013.\n",
            "Question: Are both Kurram Garhi and Trojkrsti located in the same country?\n",
            "Answer: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia.\n",
            "Thus, they are not in the same country. So the answer is no.\n",
            "Question: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same\n",
            "nationality?\n",
            "Answer: Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by Karl Freund. David\n",
            "Dhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same nationality. So the\n",
            "answer is no.\n",
            "Question: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?\n",
            "Answer: Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s father-in-law is\n",
            "Genghis Khan. So the answer is Genghis Khan.\n",
            "Question: Who was born first out of Martin Hodge and Ivania Martinich?\n",
            "Answer: Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was\n",
            "born first. So the answer is Martin Hodge.\n",
            "Question: When did the director of film Laughter In Hell die?\n",
            "Answer: The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the\n",
            "answer is August 25, 1963.\n",
            "Question: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?\n",
            "Answer: The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by\n",
            "Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus,\n",
            "the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\n",
            "Question: Who is the grandchild of Krishna Shah (Nepalese Royal)?\n",
            "Answer: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah\n",
            "has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\n",
            "Question: Which country the director of film Citizen Mavzik is from?\n",
            "Answer:Prompt D.5: exemplars of StrategyQA\n",
            "Generate a yes or no answer to the following question.\n",
            "Question: Do hamsters provide food for any animals?\n",
            "Answer: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the final\n",
            "answer is yes.\n",
            "Generate a yes or no answer to the following question.\n",
            "Question: Could Brooke Shields succeed at University of Pennsylvania?\n",
            "Answer: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the Uni-\n",
            "versity of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the final answer is yes.\n",
            "Generate a yes or no answer to the following question.\n",
            "Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?\n",
            "Answer: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number\n",
            "squared is less than 5. So the final answer is no.\n",
            "Generate a yes or no answer to the following question.\n",
            "Question: Is it common to see frost during some college commencements?\n",
            "Answer: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there\n",
            "can be frost. Thus, there could be frost at some commencements. So the final answer is yes.\n",
            "Generate a yes or no answer to the following question.\n",
            "Question: Could a llama birth twice during War in Vietnam (1945-46)?\n",
            "Answer: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months.\n",
            "Thus, a llama could not give birth twice during the War in Vietnam. So the final answer is no.\n",
            "Generate a yes or no answer to the following question.\n",
            "Question: Would a pear sink in water?\n",
            "Answer: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water float. Thus, a pear\n",
            "would float. So the final answer is no.\n",
            "Generate a yes or no answer to the following question.\n",
            "Question: Did Spiderman fight against Falcon in the MCU?\n",
            "Answer:Prompt D.6: exemplars of ASQA\n",
            "Given an ambiguous question, figure out its interpretations and answer them one by one.\n",
            "Question: Who played bonnie in gone with the wind?\n",
            "Answer: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to. In\n",
            "order to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind. Gone with\n",
            "the Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the Wind.\n",
            "Therefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who played\n",
            "Bonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with the\n",
            "Wind’s character Bonnie was played by Eleanore Cammack \"Cammie\" King. (2) The 2008 musical Gone with the Wind’s\n",
            "character Bonnie was played by Leilah de Meza.\n",
            "Given an ambiguous question, figure out its interpretations and answer them one by one.\n",
            "Question: What is the second largest city in the usa?\n",
            "Answer: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\n",
            "In order to figure out its interpretations, we need to consider different criteria to determine a city’s size. City size can be\n",
            "measured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\n",
            "city in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\n",
            "(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\n",
            "area is Juneau, Alaska.\n",
            "Given an ambiguous question, figure out its interpretations and answer them one by one.\n",
            "Question: When was bohemian rhapsody released as a single?\n",
            "Answer: This question is ambiguous in terms of which country’s release of the single is being referred to. In order to figure\n",
            "out its interpretations, we need to consider different countries where Bohemian Rhapsody is released. Bohemian Rhapsody\n",
            "was released in the United Kingdom and in the United States on different dates. Therefore, this question has 2 interpretations:\n",
            "(1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When was Bohemian Rhapsody released\n",
            "as a single in the United States? The answers to all interpretations are: (1) Bohemian Rhapsody was released as a single in the\n",
            "United Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a single in the United States on December 1975.\n",
            "Given an ambiguous question, figure out its interpretations and answer them one by one.\n",
            "Question: Where do the philadelphia eagles play their home games?\n",
            "Answer: This question is ambiguous in terms of which specific location or venue is being referred to. In order to figure out its\n",
            "interpretations, we need to consider the different possible locations or venues that could be considered the home field of the\n",
            "Philadelphia Eagles. These include the city, the sports complex, or the stadium. Therefore, this question has 3 interpretations:\n",
            "(1) What city do the Philadelphia Eagles play their home games? (2) In what sports complex do the Philadelphia Eagles play\n",
            "their home games? (3) What stadium do the Philadelphia Eagles play their home games? The answers to all interpretations\n",
            "are: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2) Philadelphia Eagles play their home games in\n",
            "the South Philadelphia Sports Complex. (3) Philadelphia Eagles play their home games in the Lincoln Financial Field stadium.\n",
            "Given an ambiguous question, figure out its interpretations and answer them one by one.\n",
            "Question: When did xbox one come out in australia?\n",
            "Answer: This question is ambiguous in terms of which specific version of the Xbox One is being referred to. In order to\n",
            "figure out its interpretations, we need to consider the different versions of the Xbox One that have been released. Xbox One\n",
            "has two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this question has 2\n",
            "interpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release in Australia? The\n",
            "answers to all interpretations are: (1) The Xbox One video game console was released in Australia on November 22, 2013.\n",
            "(2) The Xbox One X video game console was released in Australia on November 7, 2017.\n",
            "Given an ambiguous question, figure out its interpretations and answer them one by one.\n",
            "Question: When does the movie summer of 84 come out?\n",
            "Answer: This question is ambiguous in terms of which release of the movie is being referred to. In order to figure out its\n",
            "interpretations, we need to consider different releases of the movie Summer of ’84. The movie Summer of ’84 is first released\n",
            "at the Sundance Festival before it’s released throughout the US. Therefore, this question has 2 interpretations: (1) When\n",
            "did the movie Summer of ’84 first release at the Sundance Festival? (2) When did the movie Summer of ’84 first release\n",
            "throughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the Sundance Festival on\n",
            "January 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.Prompt D.7: exemplars of ASQA (cont.)\n",
            "Given an ambiguous question, figure out its interpretations and answer them one by one.\n",
            "Question: What was roy orbison’s first number one hit?\n",
            "Answer: This question is ambiguous in terms of which specific chart or region is being referred to. In order to figure out its\n",
            "interpretations, we need to consider the different charts and regions where Roy Orbison’s music was popular. Roy Orbison is\n",
            "popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this question has 2 interpretations: (1) What\n",
            "was Roy Orbison’s first number one hit in the US Hot 100 and Canada? (2) What was Roy Orbison’s first number one hit in\n",
            "the UK and Ireland? The answers to all interpretations are: (1) Running Scared was the first number one hit for Roy Orbison\n",
            "in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was the first number one hit for Roy Orbison in\n",
            "the UK and Ireland.\n",
            "Given an ambiguous question, figure out its interpretations and answer them one by one.\n",
            "Question: What is the criminal’s name in the breakfast club?\n",
            "Answer: This question is ambiguous in terms of which specific name is being referred to - the character’s name or the actor’s\n",
            "name. In order to figure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\n",
            "name. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\n",
            "What is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\n",
            "John Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\n",
            "The Breakfast Club.\n",
            "Given an ambiguous question, figure out its interpretations and answer them one by one.\n",
            "Question: How many state parks are there in virginia?\n",
            "Answer:Prompt D.8: exemplars of ASQA-hint\n",
            "Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpretations and\n",
            "answer them one by one.\n",
            "Question: Who played bonnie in gone with the wind?\n",
            "Hint: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to.\n",
            "Answer: In order to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind.\n",
            "Gone with the Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the\n",
            "Wind. Therefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who\n",
            "played Bonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with\n",
            "the Wind’s character Bonnie was played by Eleanore Cammack \"Cammie\" King. (2) The 2008 musical Gone with the Wind’s\n",
            "character Bonnie was played by Leilah de Meza.\n",
            "Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\n",
            "tions and answer them one by one.\n",
            "Question: What is the second largest city in the usa?\n",
            "Hint: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\n",
            "Answer: In order to figure out its interpretations, we need to consider different criteria to determine a city’s size. City size can\n",
            "be measured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\n",
            "city in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\n",
            "(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\n",
            "area is Juneau, Alaska.\n",
            "Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\n",
            "tions and answer them one by one.\n",
            "Question: When was bohemian rhapsody released as a single?\n",
            "Hint: This question is ambiguous in terms of which country’s release of the single is being referred to.\n",
            "Answer: In order to figure out its interpretations, we need to consider different countries where Bohemian Rhapsody is\n",
            "released. Bohemian Rhapsody was released in the United Kingdom and in the United States on different dates. Therefore,\n",
            "this question has 2 interpretations: (1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When\n",
            "was Bohemian Rhapsody released as a single in the United States? The answers to all interpretations are: (1) Bohemian\n",
            "Rhapsody was released as a single in the United Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a\n",
            "single in the United States on December 1975.\n",
            "Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\n",
            "tions and answer them one by one.\n",
            "Question: Where do the philadelphia eagles play their home games?\n",
            "Hint: This question is ambiguous in terms of which specific location or venue is being referred to.\n",
            "Answer: In order to figure out its interpretations, we need to consider the different possible locations or venues that could be\n",
            "considered the home field of the Philadelphia Eagles. These include the city, the sports complex, or the stadium. Therefore,\n",
            "this question has 3 interpretations: (1) What city do the Philadelphia Eagles play their home games? (2) In what sports\n",
            "complex do the Philadelphia Eagles play their home games? (3) What stadium do the Philadelphia Eagles play their home\n",
            "games? The answers to all interpretations are: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2)\n",
            "Philadelphia Eagles play their home games in the South Philadelphia Sports Complex. (3) Philadelphia Eagles play their\n",
            "home games in the Lincoln Financial Field stadium.\n",
            "Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\n",
            "tions and answer them one by one.\n",
            "Question: When did xbox one come out in australia?\n",
            "Hint: This question is ambiguous in terms of which specific version of the Xbox One is being referred to.\n",
            "Answer: In order to figure out its interpretations, we need to consider the different versions of the Xbox One that have been\n",
            "released. Xbox One has two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this\n",
            "question has 2 interpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release\n",
            "in Australia? The answers to all interpretations are: (1) The Xbox One video game console was released in Australia on\n",
            "November 22, 2013. (2) The Xbox One X video game console was released in Australia on November 7, 2017.\n",
            "Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\n",
            "tions and answer them one by one.\n",
            "Question: When does the movie summer of 84 come out?\n",
            "Hint: This question is ambiguous in terms of which release of the movie is being referred to.\n",
            "Answer: In order to figure out its interpretations, we need to consider different releases of the movie Summer of ’84. The\n",
            "movie Summer of ’84 is first released at the Sundance Festival before it’s released throughout the US. Therefore, this question\n",
            "has 2 interpretations: (1) When did the movie Summer of ’84 first release at the Sundance Festival? (2) When did the movie\n",
            "Summer of ’84 first release throughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the\n",
            "Sundance Festival on January 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.Prompt D.9: exemplars of ASQA-hint (cont.)\n",
            "Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpretations and\n",
            "answer them one by one.\n",
            "Question: What was roy orbison’s first number one hit?\n",
            "Hint: This question is ambiguous in terms of which specific chart or region is being referred to.\n",
            "Answer: In order to figure out its interpretations, we need to consider the different charts and regions where Roy Orbison’s\n",
            "music was popular. Roy Orbison is popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this\n",
            "question has 2 interpretations: (1) What was Roy Orbison’s first number one hit in the US Hot 100 and Canada? (2) What\n",
            "was Roy Orbison’s first number one hit in the UK and Ireland? The answers to all interpretations are: (1) Running Scared\n",
            "was the first number one hit for Roy Orbison in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was\n",
            "the first number one hit for Roy Orbison in the UK and Ireland.\n",
            "Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\n",
            "tions and answer them one by one.\n",
            "Question: What is the criminal’s name in the breakfast club?\n",
            "Hint: This question is ambiguous in terms of which specific name is being referred to - the character’s name or the actor’s\n",
            "name.\n",
            "Answer: In order to figure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\n",
            "name. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\n",
            "What is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\n",
            "John Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\n",
            "The Breakfast Club.\n",
            "Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\n",
            "tions and answer them one by one.\n",
            "Question: How many state parks are there in virginia?\n",
            "Hint: This question is ambiguous in terms of the time frame or period being referred to.\n",
            "Answer:Prompt D.10: exemplars of WikiAsp\n",
            "Generate a summary about Aslanhane Mosque including the following aspects: location, history with one aspect per line.\n",
            "# Location\n",
            "The mosque is in the old quarter of ankara next to ankara castle. With an altitude of 947 metres (3,107 ft) it overlooks ankara\n",
            "at 39°56’12\"N 32°51’55\"E.\n",
            "# History\n",
            "The mosque is one of the oldest mosques in Turkey still standing. It was built during the reign of Mesud II of the Anatolian\n",
            "Seljuks in 1290. Its architect was Ebubekir Mehmet. It was commissioned by two Ahi leaders named Hüsamettin and\n",
            "Hasaneddin. However, in 1330, it was repaired by another Ahi leader named ¸ Serafettin after whom the mosque was named.\n",
            "After several minor repairs the mosque was restored by the directorate general of foundations in 2010-2013 term.\n",
            "Generate a summary about Untold Legends: The Warrior’s Code including the following aspects: reception,\n",
            "gameplay, development with one aspect per line.\n",
            "# Reception\n",
            "The game received \"mixed or average reviews\" according to video game review aggregator Metacritic.\n",
            "# Gameplay\n",
            "The warrior’s code is a hack n’ slash action role-playing game, which concentrates on action-oriented combat.\n",
            "# Development\n",
            "As a pre-order bonus, the game was shipped with a small action figure of the Guardian class.\n",
            "Generate a summary about Raid on St. Augustine including the following aspects: aftermath, background with\n",
            "one aspect per line.\n",
            "# Aftermath\n",
            "Once the English had gone Menéndez and the rest of the Spanish settlers returned to find a smoldering ruins and very little\n",
            "left. He soon and begged for help from the viceroy of Cuba and the settlement took a while to build itself back up. The\n",
            "destroyed fort was replaced with the present day Castillo de San Marcos.\n",
            "# Background\n",
            "War had already been unofficially declared by Philip II of Spain after the Treaty of Nonsuch in which Elizabeth I had\n",
            "offered her support to the rebellious Protestant Dutch rebels. The Queen through Francis Walsingham ordered Sir Francis\n",
            "Drake to lead an expedition to attack the Spanish New World in a kind of preemptive strike. Sailing from Plymouth,\n",
            "England, he struck first at Santiago in November 1585 then across the Atlantic at the Spanish new world city of Santo\n",
            "Domingo of which was captured and ransomed on 1 January 1586 and following that successfully attacked the important\n",
            "city of Cartagena on 19 February. Drake wanted to strike at another Spanish city on the Main before finally visiting and\n",
            "replenishing Sir Walter Raleigh’s new colony of Roanoke Colony on the American East Coast. Then after this he hoped\n",
            "to make the Transatlantic crossing back to England. The fleet headed north, and in late April Drake put into the Spanish\n",
            "Cuban mainland and his men dug wells in search of fresh water and gathered supplies to help counter an outbreak of\n",
            "dysentery after which he moved on. The fleet traveled north within sight of land on the Florida peninsula sailing past\n",
            "the West coast. On 27 May 1586 as they approached further north a small fort was spotted on the shore, with a small\n",
            "inlet close by. This was the location of St Augustine, the most northerly town in Spain’s New World Empire, and the\n",
            "oldest permanent colonial settlement in North America. Drake knew of the place and was also aware of the fact that\n",
            "the spanish under Pedro Menéndez de Avilés had ordered all of the French Huguenot colonists that had tried to settle\n",
            "in the area executed. Drake decided on one final opportunity to raid and plunder, and a chance to avenge his fellow Protestants.\n",
            "Generate a summary about Lakewood (Livingston, Alabama) including the following aspects: architecture, his-\n",
            "tory with one aspect per line.\n",
            "# Architecture\n",
            "The house has a plan that is relatively rare in early Alabama architecture. The plan features a brick ground floor that is topped\n",
            "by one-and-a-half-stories of wood-frame construction. The ground floor originally contained domestic spaces, with the\n",
            "formal rooms on the principle floor and bedrooms on the upper floor. A central hallway is present on all levels. The facade is\n",
            "five bays wide, with central entrance doors on the ground and principle floors. The bays are divided by two-story Doric\n",
            "pilasters, with the middle third of the facade occupied by a two-tiered tetrastyle Doric portico. Two curved wrought iron\n",
            "staircases ascend from ground level to the front center of the upper portico, leading to the formal entrance.\n",
            "# History\n",
            "Lakewood was built for Joseph lake, a native of North Carolina, by Hiram W. Bardwell, a master builder. Construction\n",
            "was completed in 1840. Located adjacent to the University of West Alabama, Julia Strudwick Tutwiler, a Lake relative,\n",
            "periodically resided in the house from 1881 to 1910 while she served as president of the university. It was then known as\n",
            "Livingston Normal College. The house was extensively photographed by Alex Bush for the Historic American Buildings\n",
            "Survey in November and December 1936. Lakewood has continued to be owned by descendants of the Lake family to the\n",
            "current day. The house and its surviving 10 acres (4.0 ha) of grounds were listed on the Places in Peril in 2012 due to the\n",
            "immediate threat of its acquisition by developers.\n",
            "Generate a summary about Carlos Moedas including the following aspects: biography, early life, political career\n",
            "with one aspect per line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chunking the text data"
      ],
      "metadata": {
        "id": "Q8AldLrdA1p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "def split_text(text,c_size =500,c_overlap=50):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size = c_size, chunk_overlap= c_overlap)\n",
        "  chunks = text_splitter.split_text(text)\n",
        "  return chunks\n",
        "\n",
        "\n",
        "chunks = split_text(pdf_text)"
      ],
      "metadata": {
        "id": "UP5ixQp5A5sq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmUsD9UUQ9zK",
        "outputId": "d7bcde2a-b969-4e4f-fe16-44eb23e5d503"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "207"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for i in chunks:\n",
        " # print(f\"Page: \"+i)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6Wn_uZnbDmgt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding the Text Chunks"
      ],
      "metadata": {
        "id": "qyL71pRLEeLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "import torch\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "model = T5EncoderModel.from_pretrained(\"t5-base\")\n",
        "\n",
        "\n",
        "def embed_text(text):\n",
        "  inputs = tokenizer(text, return_tensors =\"pt\",padding =True,max_length =512)\n",
        "  with torch.no_grad():\n",
        "    embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
        "  return embeddings\n",
        "\n",
        "chunk_embeddings = [embed_text(chunk).squeeze().numpy() for chunk in chunks]\n",
        "\n"
      ],
      "metadata": {
        "id": "BBjURtrvEkAN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(chunk_embeddings[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XA3cQ_ODOYv7",
        "outputId": "234ad71c-7621-491c-bb64-9d43c779d2e0"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store Embeddings in FAISS"
      ],
      "metadata": {
        "id": "v_UhqVpPO0_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Initialize FAISS index\n",
        "embedding_size = chunk_embeddings[0].shape[0]\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "\n",
        "# Convert embeddings to numpy array and add them to the index\n",
        "faiss_embeddings = np.array(chunk_embeddings).astype(\"float32\")\n",
        "index.add(faiss_embeddings)"
      ],
      "metadata": {
        "id": "agt9vCLqO6Ef"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define the Retrieval"
      ],
      "metadata": {
        "id": "XsnUJ1XRSj4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_chunks(question, top_k = 3):\n",
        "  question_embedding = embed_text(question).squeeze().numpy().astype(\"float32\").reshape(1, -1)\n",
        "\n",
        "   # Search FAISS index for similar embeddings\n",
        "  distances, indices = index.search(question_embedding, top_k)\n",
        "  retrieved_chunks = [chunks[i] for i in indices[0]]\n",
        "  return retrieved_chunks\n"
      ],
      "metadata": {
        "id": "rBhdFlNlSta6"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate an Answer Using T5"
      ],
      "metadata": {
        "id": "kTH-TfkOWIty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "generation_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "\n",
        "def generate_answer(question,retrieved_chunks):\n",
        "   # Combine retrieved chunks into a single context\n",
        "   context =\" \".join(retrieved_chunks)\n",
        "   input_text = f\"question:{question} context :{context}\"\n",
        "\n",
        "\n",
        "   # Tokenize and generate answer\n",
        "   inputs = tokenizer(input_text,return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "   outputs = generation_model.generate(inputs[\"input_ids\"], max_length=150)\n",
        "   answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "   return answer"
      ],
      "metadata": {
        "id": "jpW0fZtmWLsl"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m_dVyaENinEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full question Answer function\n"
      ],
      "metadata": {
        "id": "UCKiTJOjX8qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "   # Embed chunks and store in FAISS (only needed once per PDF)\n",
        "   chunk_embeddings = [embed_text(chunk).squeeze().numpy() for chunk in chunks]\n",
        "   faiss_embeddings = np.array(chunk_embeddings).astype(\"float32\")\n",
        "   index.add(faiss_embeddings)\n",
        "\n",
        "\n",
        "\n",
        "   # Retrieve relevant chunks\n",
        "   #def retrieve_chunks(question, top_k = 3):\n",
        "   question_embedding = embed_text(question).squeeze().numpy().astype(\"float32\").reshape(1, -1)\n",
        "    # Search FAISS index for similar embeddings\n",
        "   distances, indices = index.search(question_embedding, top_k)\n",
        "   retrieved_chunks = [chunks[i] for i in indices[0]]\n",
        "\n",
        "   response = generate_answer(\"what is FLARE\",retrieved_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ubCQ1fQwjEeY",
        "outputId": "1ed4718b-473f-47fe-cae7-7756648459ce"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "handle_Index.<locals>.replacement_search() got an unexpected keyword argument 'top_k'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-45d0355d8b31>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mquestion_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m  \u001b[0;31m# Search FAISS index for similar embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mretrieved_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: handle_Index.<locals>.replacement_search() got an unexpected keyword argument 'top_k'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(pdf_path,question,top_k=3):\n",
        "   # Extract text from PDF and split into chunks\n",
        "   pdf_text = extract_text_from_pdf(pdf_path)\n",
        "   chunks = split_text(pdf_text)\n",
        "\n",
        "\n",
        "\n",
        "   # Embed chunks and store in FAISS (only needed once per PDF)\n",
        "   chunk_embeddings = [embed_text(chunk).squeeze().numpy() for chunk in chunks]\n",
        "   faiss_embeddings = np.array(chunk_embeddings).astype(\"float32\")\n",
        "   index.add(faiss_embeddings)\n",
        "\n",
        "\n",
        "\n",
        "   # Retrieve relevant chunks\n",
        "   #def retrieve_chunks(question, top_k = 3):\n",
        "   question_embedding = embed_text(question).squeeze().numpy().astype(\"float32\").reshape(1, -1)\n",
        "    # Search FAISS index for similar embeddings\n",
        "   distances, indices = index.search(question_embedding, top_k)\n",
        "   retrieved_chunks = [chunks[i] for i in indices[0]]\n",
        "    #return retrieved_chunks\n",
        "\n",
        "    #retrieveed_chunks = retrieve_chunks(question,top_k)\n",
        "\n",
        "\n",
        "   # Generate and return the answer\n",
        "   answer = generate_answer(question, retrieved_chunks)\n",
        "   return answer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BsZalVuDYAob"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tBbQKLy1i4mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/Active Retrieval Augmented Generation.pdf\"\n",
        "question = \"Single-time Retrieval Augmented Generation\"\n",
        "print(answer_question(pdf_path, question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "YMJVflSbe0Il",
        "outputId": "0e843fb6-1772-45b4-8f60-d0e5390b643b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-897880f49d6e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpdf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Active Retrieval Augmented Generation.pdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Single-time Retrieval Augmented Generation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-726457067704>\u001b[0m in \u001b[0;36manswer_question\u001b[0;34m(pdf_path, question, top_k)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Search FAISS index for similar embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m    \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m    \u001b[0mretrieved_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m#return retrieved_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-726457067704>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Search FAISS index for similar embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m    \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m    \u001b[0mretrieved_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m#return retrieved_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    }
  ]
}